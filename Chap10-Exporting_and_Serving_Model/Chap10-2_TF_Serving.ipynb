{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap10.2 - 모델 익스포트와 서빙, Serving\n",
    "\n",
    "> 텐서플로 서빙을 이용해 서버에 모델을 배포하는 방법에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 텐서플로 서빙 소개\n",
    "\n",
    "**텐서플로 서빙**(TensorFlow Serving)은 C++로 개발되었으며, Procuction 환경에서 모델을 배포할 수 있는 프레임워크이다. \n",
    "\n",
    "클라이언트(Client)가 Serving API를 통해 모델에 접근할 수 있도록 해줘, 실서비스에 사용할 수 있다. (그림 출처: [tensorflow.org](https://www.tensorflow.org/serving/))\n",
    "\n",
    "<img src=\"./images/tf_serving.jpg\" height=\"70%\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 서빙은 아래의 그림처럼 동작한다.\n",
    "\n",
    "- **Source**는 로드할 모델을 식별하는 역할을 한다.\n",
    "- 모델이 식별되면 Source는 Loader를 생성한다. Loader는 **Servable**(예측과 같은 클라이언트가 사용하는 객체) **Manager**에게 전달한다. \n",
    "- Manager는 Servable를 관리(올리기, 내리기, 서빙 등)하며, 클라이언트가 Servable에 접근할 수 있는 인터페이스를 제공한다.\n",
    "\n",
    "<img src=\"./images/tf_serving2.PNG\" height=\"70%\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

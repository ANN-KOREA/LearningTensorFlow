{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap07.2 - 텐서플로 추상화와 간소화, TFLearn\n",
    "\n",
    "> **TFLearn**은 [Chap07.1 Estimator](http://excelsior-cjh.tistory.com/157)에서 살펴본 `tf.estimator`와 마찬가지로 텐서플로의 추상화 라이브러리이다. 이번에는 TFLearn에 대해 알아보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 설치\n",
    "\n",
    "[**TFLearn**](http://tflearn.org/)은 텐서플로에 포함되어 있지 않기 때문에 별도의 설치가 필요하다. Terminal(또는 cmd창)에 `pip` 명령을 이용해 설치할 수 있다.\n",
    "\n",
    "```bash\n",
    "pip install tflearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 CNN \n",
    "\n",
    "TFLearn은 [Chap07.1 - tf.estimator](http://excelsior-cjh.tistory.com/157)와 유사하지만, TFLearn을 사용하면 조금 더 깔끔하게 모델을 만들 수 있다. [TFLearn.org](http://tflearn.org/)에서는 TFLearn을 다음과 같이 소개하고 있다.\n",
    "\n",
    "> - Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.\n",
    "- Fast prototyping through highly modular built-in neural network layers, regularizers, optimizers, metrics...\n",
    "- Full transparency over Tensorflow. All functions are built over tensors and can be used independently of TFLearn.\n",
    "- Powerful helper functions to train any TensorFlow graph, with support of multiple inputs, outputs and optimizers.\n",
    "- Easy and beautiful graph visualization, with details about weights, gradients, activations and more...\n",
    "- Effortless device placement for using multiple CPU/GPU.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFLearn에서의 모델 생성은 `regression()`을 사용하여 래핑되고 마무리된다. `regression()`함수에서 손실함수(`loss`) 및 최적화(`optimizer`)를 설정해준다.\n",
    "\n",
    "그렇다면, TFLearn을 이용해 MNIST 데이터를 분류하는 CNN 모델을 만들어 보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.10814\u001b[0m\u001b[0m | time: 5.414s\n",
      "| Adam | epoch: 003 | loss: 0.10814 - acc: 0.9841 -- iter: 54976/55000\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.10413\u001b[0m\u001b[0m | time: 6.558s\n",
      "| Adam | epoch: 003 | loss: 0.10413 - acc: 0.9826 | val_loss: 0.04778 - val_acc: 0.9861 -- iter: 55000/55000\n",
      "--\n",
      "INFO:tensorflow:/D/dev/LearningTensorFlow/Chap07-TF_Abstractions/MNIST_tflearn_checkpoints/checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/D/dev/LearningTensorFlow/Chap07-TF_Abstractions/MNIST_tflearn_checkpoints/checkpoint-2580 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# 데이터를 로딩하고 기본적인 변환을 수행\n",
    "train_x, train_y, test_x, test_y = mnist.load_data(one_hot=True, \n",
    "                                                   data_dir='../data')\n",
    "train_x = train_x.reshape([-1, 28, 28, 1])\n",
    "test_x = test_x.reshape([-1, 28, 28, 1])\n",
    "\n",
    "# Building the network\n",
    "CNN = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "CNN = conv_2d(CNN, 32, 5, activation='relu', regularizer=\"L2\")\n",
    "CNN = max_pool_2d(CNN, 2)\n",
    "CNN = local_response_normalization(CNN)\n",
    "CNN = conv_2d(CNN, 64, 5, activation='relu', regularizer='L2')\n",
    "CNN = max_pool_2d(CNN, 2)\n",
    "CNN = local_response_normalization(CNN)\n",
    "CNN = fully_connected(CNN, 1024, activation=None)\n",
    "CNN = dropout(CNN, 0.5)\n",
    "CNN = fully_connected(CNN, 10, activation='softmax')\n",
    "CNN = regression(CNN, optimizer='adam', learning_rate=0.0001, \n",
    "                 loss='categorical_crossentropy', name='target')\n",
    "\n",
    "# Training the network\n",
    "model = tflearn.DNN(CNN, tensorboard_verbose=0, \n",
    "                    tensorboard_dir='./MNIST_tflearn_board/', \n",
    "                    checkpoint_path='./MNIST_tflearn_checkpoints/checkpoint')\n",
    "model.fit({'input': train_x}, {'target': train_y}, n_epoch=3,\n",
    "          validation_set=({'input': test_x}, {'target': test_y}),\n",
    "          snapshot_step=1000, show_metric=True, run_id='convnet_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서 `tflearn.DNN()`함수는 `tf.estimator.Estimator()`와 비슷한 기능을 하는데, `regression()`으로 래핑된 모델을 인스턴스화하고 만들어진 모델을 전달하는 역할을 한다. 또한 텐서보드(TensorBoard)와 체크포인트(checkpoint) 디렉터리 등을 설정할 수 있다. 모델 적합 연산은 `.fit()` 메서드를 이용해 수행된다. \n",
    "\n",
    "모델 적합(`.fit()`), 즉 학습이 완료되면, 다음과 같은 메소드를 이용해 모델을 평가, 예측, 저장 및 불러오기 등을 수행할 수 있다.\n",
    "\n",
    "| 메서드                         | 설명                                               |\n",
    "| ------------------------------ | -------------------------------------------------- |\n",
    "| `evaluate(X, Y, batch_size)`   | 주어진 샘플에서 모델을 평가                        |\n",
    "| `fit(X, Y, n_epoch)`           | 입력 feature `X`와 타겟 `Y`를 모델에 적용하여 학습 |\n",
    "| `get_weights(weight_tensor)`   | 변수의 가중치를 반환                               |\n",
    "| `load(model_file)`             | 학습된 모델 가중치를 불러오기                      |\n",
    "| `predict(x)`                   | 주어진 `x` 데이터를 모델을 이용해 예측             |\n",
    "| `save(model_file)`             | 학습된 모델 가중치를 저장                          |\n",
    "| `set_weights(tensor, weights)` | 주어진 값을 텐서 변수에 할당                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9861]\n",
      "0.9861\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "evaluation = model.evaluate(X=test_x, Y=test_y, batch_size=128)\n",
    "print(evaluation)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(test_x)\n",
    "accuarcy = (np.argmax(pred, 1)==np.argmax(test_y, 1)).mean()\n",
    "print(accuarcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3. RNN\n",
    "\n",
    "이번에는 TFLearn을 이용해 RNN을 구현해 보도록하자. 구현할 RNN 모델은 영화 리뷰에 대한 감성분석으로, 리뷰에 대해 좋거나/나쁘거나 두 개의 클래스를 분류하는 모델이다. 데이터는 학습 및 테스트 데이터가 각각 25,000개로 이루어진 [IMDb](https://www.imdb.com/interfaces/) 리뷰 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# IMDb 데이터셋 로드\n",
    "(train_x, train_y), (test_x, test_y), _ = imdb.load_data(path='../data/imdb.pkl', \n",
    "                                                         n_words=10000,\n",
    "                                                         valid_portion=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 불러온 IMDb 데이터는 각각 다른 시퀀스 길이를 가지고 있으므로 최대 시퀀스 길이를 100으로 하여 `tflearn.data_utils.pad_sequences()`를 사용해 제로 패딩으로 시퀀스의 길이를 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence padding and Converting labels to binary vectors\n",
    "train_x = pad_sequences(train_x, maxlen=100, value=0.)\n",
    "test_x = pad_sequences(test_x, maxlen=100, value=0.)\n",
    "train_y = to_categorical(train_y, nb_classes=2)\n",
    "test_y = to_categorical(test_y, nb_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런다음, `tflearn.embedding()`으로 벡터 공간으로의 임베딩을 수행한다. 아래의 코드에서 확인할 수 있듯이 각 단어는 128 크기인 벡터에 매핑된다. 이렇게 임베딩된 결과를 `LSTM` layer와 `fully_connected` layer를 추가해 모델을 구성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m0.05996\u001b[0m\u001b[0m | time: 22.310s\n",
      "| Adam | epoch: 010 | loss: 0.05996 - acc: 0.9827 -- iter: 22496/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.05474\u001b[0m\u001b[0m | time: 23.425s\n",
      "| Adam | epoch: 010 | loss: 0.05474 - acc: 0.9845 | val_loss: 0.85953 - val_acc: 0.8064 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Building a LSTM network\n",
    "# Embedding\n",
    "RNN = tflearn.input_data([None, 100])\n",
    "RNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)\n",
    "\n",
    "# LSTM Cell\n",
    "RNN = tflearn.lstm(RNN, 128, dropout=0.8)\n",
    "RNN = tflearn.fully_connected(RNN, 2, activation='softmax')\n",
    "RNN = tflearn.regression(RNN, optimizer='adam', \n",
    "                         learning_rate=0.001, loss='categorical_crossentropy')\n",
    "\n",
    "# Training the network\n",
    "model = tflearn.DNN(RNN, tensorboard_verbose=0, \n",
    "                    tensorboard_dir='./IMDb-tflearn_board/')\n",
    "model.fit(train_x, train_y, \n",
    "          validation_set=(test_x, test_y), \n",
    "          show_metric=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8063999997138978]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "evaluation = model.evaluate(test_x, test_y, batch_size=128)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드는 위의 코드를 합친 전체 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# IMDb 데이터셋 로드\n",
    "(train_x, train_y), (test_x, test_y), _ = imdb.load_data(path='../data/imdb.pkl', \n",
    "                                                         n_words=10000,\n",
    "                                                         valid_portion=0.1)\n",
    "\n",
    "\n",
    "# Sequence padding and Converting labels to binary vectors\n",
    "train_x = pad_sequences(train_x, maxlen=100, value=0.)\n",
    "test_x = pad_sequences(test_x, maxlen=100, value=0.)\n",
    "train_y = to_categorical(train_y, nb_classes=2)\n",
    "test_y = to_categorical(test_y, nb_classes=2)\n",
    "\n",
    "# Building a LSTM network\n",
    "# Embedding\n",
    "RNN = tflearn.input_data([None, 100])\n",
    "RNN = tflearn.embedding(RNN, input_dim=10000, output_dim=128)\n",
    "\n",
    "# LSTM Cell\n",
    "RNN = tflearn.lstm(RNN, 128, dropout=0.8)\n",
    "RNN = tflearn.fully_connected(RNN, 2, activation='softmax')\n",
    "RNN = tflearn.regression(RNN, optimizer='adam', \n",
    "                         learning_rate=0.001, loss='categorical_crossentropy')\n",
    "\n",
    "# Training the network\n",
    "model = tflearn.DNN(RNN, tensorboard_verbose=0, \n",
    "                    tensorboard_dir='./IMDb-tflearn_board/')\n",
    "model.fit(train_x, train_y, \n",
    "          validation_set=(test_x, test_y), \n",
    "          show_metric=True, batch_size=32)\n",
    "\n",
    "# evaluate the network\n",
    "evaluation = model.evaluate(test_x, test_y, batch_size=128)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 정리\n",
    "\n",
    "이번 포스팅에서는 TFLearn을 살펴보고, 이를 이용해 CNN과 RNN을 구현해 보았다. 이 외에도 TFLearn에 대한 사용법 및 예제는 http://tflearn.org/ 에서 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

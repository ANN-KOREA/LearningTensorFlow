{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap07 - 텐서플로 추상화와 간소화\n",
    "\n",
    "> 추상화가 무엇이며 왜 유용한지 알아보고 텐서플로(TensorFlow)의 몇몇 대중적인 추상화 라이브러리를 간단하게 살펴보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 개요\n",
    "\n",
    "**추상화(abstraction)**이란 코드를 특정한 목적으로 일반화하여 기존 코드 **'위에 올라가는'** 코드의 계층을 의미한다. 관련있는 High-Level 기능을 묶는 방식의 재구성을 통해 코드를 감싸서 추상화한다. 따라서 쉽게 코딩할 수 있고, 가독성이 좋으며, 코드가 간소화된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 추상화 라이브러리 둘러보기\n",
    "\n",
    "텐서플로(TensorFlow)에서 사용할 수 있는 인기있는 추상화 라이브러리는 다음과 같다고 한다.\n",
    "\n",
    "- `tf.estimator`\n",
    "- TFLearn\n",
    "- TF-Slim\n",
    "- Keras\n",
    "\n",
    "TFLearn은 별도의 설치가 필요하며 `tf.estimator`과 TF-Slim([`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/r1.8/tensorflow/contrib/slim))은 텐서플로에 포함되어 있다. Keras(케라스)는 2017년 구글의 공식 후원을 받아 1.1버전 부터 `tf.contrib.keras`안에 들어왔으며, 현재 이글을 작성하는 시점인 2018.06.27의 1.8버전에는 `tf.keras`로 변경됭 텐서플로의 한 부분으로 자리를 잡았다. `contrib`안의 라이브러리들은 **기부된(contributed)** 것을 의미하며 정식으로 반영되기 전까지 테스트가 더 필요하다는 것을 의미한다. \n",
    "\n",
    "`tf.estimator`은 Python의 대표적인 머신러닝 모듈인 [Scikit-Learn](http://scikit-learn.org/stable/index.html)(`sklearn`)의 스타일처럼 복잡한 딥러닝 모델을 쉽게 작성할 수 있도록 해주는 라이브러리다. Keras와 마찬가지로 [`tf.contrib.learn`](https://github.com/tensorflow/tensorflow/tree/r1.8/tensorflow/contrib/learn)으로 텐서플로에 들어왔으며, 현재 1.8버전(2018.06.27)에는 `tf.estimator`로 옮겨졌으며 `tf.contrib.learn`은 유지되고 있지만 사용할 경우 `deprecated` 경고가 나타난다.\n",
    "\n",
    "TF-Slim은 주로 복잡한 합성곱 신경망(Convolutional Networks)을 쉽게 설계할 수 있도록 만들어졌으며 다양한 Pre-Trained 모델을 제공해 학습 시간을 단축시킬 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 tf.estimator\n",
    "\n",
    "`tf.estimator`는 `sklearn`과 유사하게 `Estimator`(모델들을 부르는 용어)로 동작하는데 이것을 이용하면, 빠르게 모델을 학습할 수 있다. 대표적인 Estimator들은 아래와 같으며, [`tf.estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator)에서 다양한 Estimator들을 살펴볼 수 있다.\n",
    "\n",
    "| Estimator                                                    | 설명                                                         |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| [`LinearRegressor()`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor) | 주어진 관측치에 대해 Linear regression을 이용해 예측         |\n",
    "| [`LinearClassifier()`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier) | Linear Model을 이용한 다중분류(multi-classification), 클래스가 2개일 경우 binary classification |\n",
    "| [`DNNRegressor()`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) | TensorFlow DNN 모델을 이용한 regression                      |\n",
    "| [`DNNClassifier()`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) | TensorFlow DNN 모델을 이용한 classification                  |\n",
    "\n",
    "`tf.contrib.learn`에서 `tf.estimator`로 옮겨올때 아래의 `Estimator`는 반영이 되지 않았다.\n",
    "\n",
    "- `DynamicRnnEstimator`: Consider a custom `model_fn`.\n",
    "- `KMeansClustering`: Use `tf.contrib.factorization.KMeansClustering`.\n",
    "- `LogisticRegressor`: Not supported. Instead, use `binary_classification_head` with a custom `model_fn`, or with `DNNEstimator`.\n",
    "- `StateSavingRnnEstimator`: Consider a custom `model_fn`.\n",
    "- SVM: Consider a custom `model_fn`.\n",
    "- `LinearComposableModel` and `DNNComposableModel`: Not supported. Consider `tf.contrib.estimator.DNNEstimator`, or write a custom model_fn.\n",
    "- `MetricSpec`: Deprecated. For adding custom metrics to canned Estimators, use `tf.contrib.estimator.add_metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 Estimator를 사용하는 방법은 다음과 같다.\n",
    "\n",
    "1. 클래스 **인스턴스화**(instance)\n",
    "    - `model = tf.estimator.<some_Estimator>()`\n",
    "2. 학습 데이터를 사용해 모델을 **학습** \n",
    "    - `model.train()`\n",
    "3. 학습된 모델을 **평가**(evaluate)\n",
    "    - `model.evaluate()`\n",
    "4. 테스트 데이터를 이용해 결과를 **예측**(predict)\n",
    "    - `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 선형회귀 - Linear Regression\n",
    "\n",
    "선형회귀(linear regression)은 데이터 집합 $\\left\\{ y_i, x_{i1}, \\dots , x_{ip} \\right\\}_{i=1}^{n}$ 에 대해, 종속변수 $y_i$와 $p$개의 설명변수 $x_i$ 사이의 선형 관계를 모델링한다(출처: [wikipedia](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)).\n",
    "\n",
    "$$\n",
    "y_i = w_1 x_{i1} + \\cdots + w_{p} x_{ip} + \\epsilon_i = \\mathbf{x}_{i}^{T} \\mathbf{W} + \\epsilon_i, \\quad i = 1, \\dots , n\n",
    "$$\n",
    "\n",
    "위의 선형회귀 모델을 Scikit-Learn(사이킷런)에서 제공하는 [보스턴 하우징(Boston Housing)](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)데이터에 적용해보도록 하자. 먼저 추상화 라이브러리를 사용하지 않은 텐서플로(TensorFlow)코드를 구현한 뒤, 이것을 `tf.estimator.LiearRegressor()`를 이용해 구현해 볼 것이다.\n",
    "\n",
    "보스턴 하우징 데이터 설명은 다음과 같다. 종속변수(타겟변수)는 소유자가 거주 중인 주택 가격의 중간값(median)이다.\n",
    "\n",
    "|      | Feature | 설명                                                      |\n",
    "| ---- | ------- | --------------------------------------------------------- |\n",
    "| 1    | CRIM    | 마을의 1인당 범죄율                                       |\n",
    "| 2    | ZN      | 2만 5천 평방 피트 이상으로 구획된 택지의 비율             |\n",
    "| 3    | INDUS   | 마을당 비상업 업무 지구의 비율                            |\n",
    "| 4    | CHAS    | 찰스강 더미 변수 (인접: 1, 아니면: 0)                     |\n",
    "| 5    | NOX     | 질소산화물 농도                                           |\n",
    "| 6    | RM      | 주택당 평균 방의 수                                       |\n",
    "| 7    | AGE     | 1940년 이전에 지어진 건물의 비율                          |\n",
    "| 8    | DIS     | 5개의 보스턴 고용 센터까지의 가중 거리                    |\n",
    "| 9    | RAD     | 방사형 고속도로까지의 접근성 지수                         |\n",
    "| 10   | TAX     | 1만 달러당 부가가치세                                     |\n",
    "| 11   | PTRATIO | 마을별 학생 대 교사 비율                                  |\n",
    "| 12   | B       | $1000(\\text{Bk}-0.63)^{2}$, $\\text{Bk}$: 마을의 흑인 비율 |\n",
    "| 13   | LSTAT   | 하위 계층의 비율(%)                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 Scikit-Learn에서 보스턴 하우징 데이터를 불러온다. 설명변수로 쓰일 데이터들을 [`StandardScaler()`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)를 이용해 정규화 해준다. `StandardScaler`의 수식은 다음과 같으며 *Z-Score*라고도 한다.\n",
    "\n",
    "$$\n",
    "z = \\frac{\\text{Data point} - \\text{mean}}{\\text{standard deviation}} = \\frac{x-\\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape : (506, 13)\n",
      "y_data.shape : (506,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, metrics, preprocessing\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x_data = preprocessing.StandardScaler().fit_transform(boston.data)\n",
    "y_data = boston.target\n",
    "\n",
    "print('x_data.shape :', x_data.shape)\n",
    "print('y_data.shape :', y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natvie TensorFlow\n",
    "\n",
    "먼저 추상화 라이브러리를 사용하지 않고, 순수 텐서플로를 이용해 선형회귀를 모델링 해보도록 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 40\t MSE: 22.30192\n",
      "Step: 80\t MSE: 22.00200\n",
      "Step: 120\t MSE: 21.93284\n",
      "Step: 160\t MSE: 21.91024\n",
      "Step: 200\t MSE: 21.90225\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float64, shape=(None, 13))\n",
    "y_true = tf.placeholder(tf.float64, shape=(None))\n",
    "\n",
    "with tf.name_scope('inference'):\n",
    "    w = tf.Variable(tf.zeros([1, 13], dtype=tf.float64, name='weights'))\n",
    "    b = tf.Variable(0, dtype=tf.float64, name='bias')\n",
    "    y_pred = tf.matmul(w, tf.transpose(x)) + b\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y_true-y_pred))  # MSE\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = 0.1\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(200):\n",
    "        MSE, _ = sess.run([loss, train], feed_dict={x: x_data, \n",
    "                                                    y_true: y_data})\n",
    "    \n",
    "        if (step+1) % 40 == 0:\n",
    "            print('Step: {:2d}\\t MSE: {:.5f}'.format(step+1, MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.estimator\n",
    "\n",
    "이번에는 위와 똑같은 기능을 하는 코드를 `tf.estimator.LinearRegressor()`를 이용해 구현해 보도록 하자. 아래의 코드에서 확인할 수 있듯이 간단한 코드로 Linear Regression을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0c3d9cd908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 299626.34, step = 0\n",
      "INFO:tensorflow:Loss for final step: 299626.34.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-27-10:02:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-27-10:02:49\n",
      "INFO:tensorflow:Saving dict for global step 1: average_loss = 879.69196, global_step = 1, loss = 445124.12\n",
      "{'average_loss': 879.69196, 'loss': 445124.12, 'global_step': 1}\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 200\n",
    "MINIBATCH_SIZE = 506\n",
    "\n",
    "feature_column = [tf.feature_column.numeric_column(key='x', shape=13)]\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        {'x': x_data}, y_data, batch_size=MINIBATCH_SIZE, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        {'x': x_data}, y_data, batch_size=MINIBATCH_SIZE, shuffle=False)\n",
    "\n",
    "reg = tf.estimator.LinearRegressor(\n",
    "        feature_columns=feature_column,\n",
    "        optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001), \n",
    "        model_dir='./model')\n",
    "\n",
    "reg.train(input_fn=train_input_fn, steps=NUM_STEPS)\n",
    "MSE = reg.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 DNN 분류기\n",
    "\n",
    "이번에는 [2.4 - Softmax Regression](http://excelsior-cjh.tistory.com/149?category=940399)에서 구현한 MNIST 분류기를 `tf.estimator.DNNClassifier()`를 이용해 구현해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# MNIST 데이터 불러오기 위한 함수 정의\n",
    "def mnist_load():\n",
    "    (train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Train set\n",
    "    train_x = train_x.astype('float32') / 255.\n",
    "    train_y = train_y.astype('int32')\n",
    "    # Test set\n",
    "    test_x = test_x.astype('float32') / 255.\n",
    "    test_y = test_y.astype('int32')\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# MNIST 데이터 불러오기\n",
    "(train_x, train_y), (test_x, test_y) = mnist_load()\n",
    "\n",
    "# Define train_input_fn\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': train_x}, y=train_y, shuffle=True, batch_size=MINIBATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0bca1f3908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1178.339, step = 0\n",
      "INFO:tensorflow:global_step/sec: 298.774\n",
      "INFO:tensorflow:loss = 121.545364, step = 100 (0.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 119 into ./model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 95.376915.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f0bca1f3780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_STEPS = 5000\n",
    "MINIBATCH_SIZ = 128\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column('x', shape=[28, 28])]\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[200],\n",
    "    n_classes=10,\n",
    "    optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.2),\n",
    "    model_dir='./model'\n",
    ")\n",
    "\n",
    "dnn.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-27-10:03:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-119\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-27-10:03:23\n",
      "INFO:tensorflow:Saving dict for global step 119: accuracy = 0.9453125, average_loss = 0.2138238, global_step = 119, loss = 27.369446\n",
      "test accuracy: 0.9453125\n"
     ]
    }
   ],
   "source": [
    "# Define eval_input_fn\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'x': test_x}, y=test_y, shuffle=False\n",
    ")\n",
    "\n",
    "test_acc = dnn.evaluate(input_fn=eval_input_fn, steps=1)['accuracy']\n",
    "print('test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 tf.feature_column\n",
    "\n",
    "텐서플로는 [`tf.feature_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column)를 통해 다양한 데이터의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
